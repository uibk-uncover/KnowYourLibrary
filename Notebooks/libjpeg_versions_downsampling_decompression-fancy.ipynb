{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272386cf",
   "metadata": {},
   "source": [
    "# Analysis of libjpeg - downsampling\n",
    "\n",
    "**Author:** Martin BeneÅ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d3557",
   "metadata": {},
   "source": [
    "This notebook contains forensic analysis of chroma upsampling in various libjpeg versions for decompression, where fancy upsampling is used in both compression and decompression. Both RGB and grayscale are tested separately. All the other parameters are kept default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc874b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions to test\n",
    "versions = ['6b','turbo210','7','8','8a','8b','8c','8d','9','9a','9b','9c','9d','9e']\n",
    "versions2 = [v[:min(len(v),5)] for v in versions]\n",
    "\n",
    "# default versions\n",
    "import jpeglib\n",
    "v_arbitrary = '7' # arbitrary version for compression\n",
    "jpeglib.version.set(v_arbitrary)\n",
    "c_flags = ['+DO_FANCY_UPSAMPLING']\n",
    "\n",
    "# random subsample size\n",
    "N_samples = 10\n",
    "\n",
    "# database path\n",
    "from pathlib import Path\n",
    "db_path = Path.home() / 'Datasets'\n",
    "\n",
    "# sampling factor\n",
    "samp_factors = [\n",
    "    ((1,1),(1,1),(1,1)), # 4:4:4\n",
    "    ((1,2),(1,2),(1,2)),\n",
    "    ((2,1),(2,1),(2,1)),\n",
    "    \n",
    "    ((1,2),(1,1),(1,1)), # 4:4:0\n",
    "    ((2,2),(2,1),(2,1)),\n",
    "    ((1,4),(1,2),(1,2)),\n",
    "    ((1,2),(1,2),(1,1)),   # Cb 4:4:4 Cr 4:4:0\n",
    "    ((1,2),(1,1),(1,2)),   # Cb 4:4:0 Cr 4:4:4\n",
    "    \n",
    "    ((2,1),(1,1),(1,1)), # 4:2:2\n",
    "    ((2,2),(1,2),(1,2)),\n",
    "    ((2,1),(2,1),(1,1)),   # Cb 4:4:4 Cr 4:2:2\n",
    "    ((2,1),(1,1),(2,1)),   # Cb 4:2:2 Cr 4:4:4\n",
    "    \n",
    "    ((2,2),(1,1),(1,1)), # 4:2:0\n",
    "    ((2,2),(2,1),(1,1)),   # Cb 4:4:0 Cr 4:2:0\n",
    "    ((2,2),(1,1),(2,1)),   # Cb 4:2:0 Cr 4:4:0\n",
    "    ((2,2),(1,2),(1,1)),   # Cb 4:2:2 Cr 4:2:0\n",
    "    ((2,2),(1,1),(1,2)),   # Cb 4:2:0 Cr 4:2:2\n",
    "    ((2,2),(2,2),(1,1)),   # Cb 4:4:4 Cr 4:2:0\n",
    "    ((2,2),(2,2),(2,1)),   # Cb 4:4:4 Cr 4:4:0\n",
    "    ((2,2),(2,2),(1,2)),   # Cb 4:4:4 Cr 4:2:2\n",
    "    ((2,2),(1,1),(2,2)),   # Cb 4:2:0 Cr 4:4:4\n",
    "    ((2,2),(2,1),(2,2)),   # Cb 4:4:0 Cr 4:4:4\n",
    "    ((2,2),(1,2),(2,2)),   # Cb 4:2:2 Cr 4:4:4\n",
    "    \n",
    "    ((4,1),(1,1),(1,1)), # 4:1:1\n",
    "    ((4,1),(2,1),(1,1)),   # Cb 4:2:2 Cr 4:1:1\n",
    "    ((4,1),(1,1),(2,1)),   # Cb 4:1:1 Cr 4:2:2\n",
    "    \n",
    "    ((4,2),(1,1),(1,1)), # 4:1:0\n",
    "    \n",
    "    ((1,4),(1,1),(1,1)), # 1:0.5:0\n",
    "    ((1,4),(1,2),(1,1)),\n",
    "    \n",
    "    ((2,4),(1,1),(1,1)), # 2:0.5:0\n",
    "    \n",
    "    ((3,1),(1,1),(1,1)), # 3:1:1\n",
    "    ((3,1),(3,1),(1,1)),   # Cb 4:4:4 Cr 3:1:1\n",
    "    ((3,1),(1,1),(3,1)),   # Cb 3:1:1 Cr 4:4:4\n",
    "    ((3,2),(3,1),(1,1)), # 3:3:0\n",
    "    ((3,2),(1,2),(1,2)), # 3:1:1\n",
    "]\n",
    "\n",
    "# checkerboard\n",
    "import numpy as np\n",
    "def checkerboard(boardsize, tilesize, channels=3):\n",
    "    board = np.zeros([*boardsize, channels], dtype=np.uint8)\n",
    "    for i in range(boardsize[0]):\n",
    "        for j in range(boardsize[1]):\n",
    "            if (i//tilesize[0]) % 2 == (j//tilesize[1]) % 2:\n",
    "                board[i,j] = 255\n",
    "    return board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6a14c",
   "metadata": {},
   "source": [
    "## Load ALASKA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb690455",
   "metadata": {},
   "source": [
    "Load ALASKA2 database consisting of 70000 colored images. In this case we have uncompressed version of shape 256x256. You can find the scripts to download it [here](https://alaska.utt.fr/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55e22c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALASKA2 database with 80010 images.\n",
      "Input shape (15, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load ALASKA2 database\n",
    "import os\n",
    "alaska_path = db_path / 'ALASKA_v2_TIFF_256_COLOR'\n",
    "alaska_names = [alaska_path / f for f in os.listdir(alaska_path)]\n",
    "print(\"Loaded ALASKA2 database with\", len(alaska_names), \"images.\")\n",
    "\n",
    "# sample without replacement\n",
    "import random\n",
    "random.seed(42) # answer to everything\n",
    "alaska_names_sub = random.sample(alaska_names, N_samples-2)\n",
    "\n",
    "# choose most and least saturated\n",
    "import matplotlib.pyplot as plt\n",
    "#most,least = (None,0),(None,0)\n",
    "#for i,f in enumerate(alaska_names):\n",
    "#    if i % 500 == 0: print(i, '/', len(alaska_names), '         ', end='\\r')\n",
    "#    if str(f).split('.')[-1] != 'tif': continue\n",
    "#    x = plt.imread(str(f))\n",
    "#    xmin,xmax = (x == 0).sum(),(x == 255).sum()\n",
    "#    if xmin > least[1]: least = (f,xmin)\n",
    "#    if xmax > most[1]: most = (f,xmax)\n",
    "most,least = (alaska_path / '10343.tif',98491),(alaska_path / '05887.tif', 78128)\n",
    "# add them\n",
    "alaska_names_sub.append(most[0])\n",
    "alaska_names_sub.append(least[0])\n",
    "\n",
    "# load the image with PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "alaska = np.array([plt.imread(f) for f in alaska_names_sub])\n",
    "\n",
    "# append checkerboard\n",
    "for tilesize in [(4,4),(7,7),(8,8),(15,15),(16,16)]:\n",
    "    alaska = np.append(alaska, np.expand_dims(checkerboard((256, 256), tilesize, 3), 0), 0)\n",
    "\n",
    "print(\"Input shape\", alaska.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f3818",
   "metadata": {},
   "source": [
    "## Decompression with fancy upsampling in decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d73d8d",
   "metadata": {},
   "source": [
    "### Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a154e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images recompressed by each version\n",
    "import tempfile\n",
    "images_rgb = {'version': [], 'samp_factor': [], 'Y': [], 'Cb': [], 'Cr': [], 'image': []}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    \n",
    "    # iterate versions\n",
    "    for i,v_decompress in enumerate(versions):\n",
    "        \n",
    "        # iterate samp factors\n",
    "        for samp_factor in samp_factors:\n",
    "            \n",
    "            # compress each image with version\n",
    "            fnames = [str(Path(tmp) / f'{i}.jpeg') for i in range(alaska.shape[0])]\n",
    "            with jpeglib.version(v_arbitrary):\n",
    "                for i,fname in enumerate(fnames):\n",
    "                    im = jpeglib.from_spatial(alaska[i])\n",
    "                    im.samp_factor = samp_factor\n",
    "                    im.write_spatial(fname, flags=c_flags)\n",
    "        \n",
    "            # decompress with single (arbitrary) version\n",
    "            with jpeglib.version(v_decompress):\n",
    "                images_rgb['version'].append(v_decompress)\n",
    "                images_rgb['samp_factor'].append(samp_factor)\n",
    "                images_rgb['image'].append(np.array([\n",
    "                    jpeglib.read_spatial(fname, flags=['+DO_FANCY_UPSAMPLING']).spatial for fname in fnames\n",
    "                ]))\n",
    "                images_rgb['Y'].append([\n",
    "                    jpeglib.read_dct(fname).Y for fname in fnames\n",
    "                ])\n",
    "                images_rgb['Cb'].append([\n",
    "                    jpeglib.read_dct(fname).Cb for fname in fnames\n",
    "                ])\n",
    "                images_rgb['Cr'].append([\n",
    "                    jpeglib.read_dct(fname).Cr for fname in fnames\n",
    "                ])\n",
    "\n",
    "# dataframe\n",
    "import pandas as pd\n",
    "images_rgb = pd.DataFrame(images_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87cfdb",
   "metadata": {},
   "source": [
    "### N-to-N comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 1), (1, 1), (1, 1)) False False False | DCT True True\n",
      "((1, 2), (1, 2), (1, 2)) False False False | DCT True True\n",
      "((2, 1), (2, 1), (2, 1)) False False False | DCT True True\n",
      "((1, 2), (1, 1), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (2, 1), (2, 1)) False False False | DCT True True\n",
      "((1, 4), (1, 2), (1, 2)) False False False | DCT True True\n",
      "((1, 2), (1, 2), (1, 1)) False False False | DCT True True\n",
      "((1, 2), (1, 1), (1, 2)) False False False | DCT True True\n",
      "((2, 1), (1, 1), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (1, 2), (1, 2)) False False False | DCT True True\n",
      "((2, 1), (2, 1), (1, 1)) False False False | DCT True True\n",
      "((2, 1), (1, 1), (2, 1)) False False False | DCT True True\n",
      "((2, 2), (1, 1), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (2, 1), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (1, 1), (2, 1)) False False False | DCT True True\n",
      "((2, 2), (1, 2), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (1, 1), (1, 2)) False False False | DCT True True\n",
      "((2, 2), (2, 2), (1, 1)) False False False | DCT True True\n",
      "((2, 2), (2, 2), (2, 1)) False False False | DCT True True\n",
      "((2, 2), (2, 2), (1, 2)) False False False | DCT True True\n"
     ]
    }
   ],
   "source": [
    "# L1 distance metric\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "mismatch = lambda x1,x2: (np.abs(x1.astype(np.int32) - x2.astype(np.int32)) != 0).mean()\n",
    "\n",
    "# get distance matrix\n",
    "def get_distmat(x):\n",
    "    # images to distance matrix\n",
    "    images_x_list = np.array([list(i) for i in x.to_list()], dtype=object)\n",
    "    images_x_list = images_x_list[:80,:80] # to fasten the computation\n",
    "    images_x_list = images_x_list.reshape(len(versions), -1)\n",
    "    dists_x = pdist(images_x_list, mismatch)\n",
    "    return squareform(dists_x)\n",
    "\n",
    "# images by sampling factor\n",
    "distmats_rgb = {}\n",
    "distmats_Y = {}\n",
    "distmats_Cb = {}\n",
    "distmats_Cr = {}\n",
    "for samp_factor in samp_factors:\n",
    "    images_rgb_sf = images_rgb[images_rgb.samp_factor == samp_factor]\n",
    "    \n",
    "    # images to distance metric\n",
    "    distmats_rgb[samp_factor] = get_distmat(images_rgb_sf.image)\n",
    "    distmats_Y[samp_factor] = get_distmat(images_rgb_sf.Y)\n",
    "    distmats_Cb[samp_factor] = get_distmat(images_rgb_sf.Cb)\n",
    "    distmats_Cr[samp_factor] = get_distmat(images_rgb_sf.Cr)\n",
    "    \n",
    "    print(\n",
    "        samp_factor,\n",
    "        ((distmats_rgb[samp_factor] == 0) == (distmats_Y[samp_factor] == 0)).all(),\n",
    "        ((distmats_rgb[samp_factor] == 0) == (distmats_Cb[samp_factor] == 0)).all(),\n",
    "        ((distmats_rgb[samp_factor] == 0) == (distmats_Cr[samp_factor] == 0)).all(),\n",
    "        \"| DCT\",\n",
    "        ((distmats_Y[samp_factor] == 0) == (distmats_Cb[samp_factor] == 0)).all(),\n",
    "        ((distmats_Y[samp_factor] == 0) == (distmats_Cr[samp_factor] == 0)).all(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087f1cd",
   "metadata": {},
   "source": [
    "We detect differences for all sampling factor - an indicator for decompression mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b280b9b",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29753eba",
   "metadata": {},
   "source": [
    "First is to check that in DCT all the versions are identical. That means that nothing strange happens in compression (which shouldn't because we use the same version in all cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# cluster by sampling factor\n",
    "for samp_factor in samp_factors:\n",
    "    for k in range(1,6):\n",
    "        agnes = AgglomerativeClustering(n_clusters=k, linkage='average', affinity='precomputed')\n",
    "        agnes.fit(distmats_Y[samp_factor])\n",
    "    \n",
    "        # compute heterogenity metric (sum of distances)\n",
    "        heterogenity = np.sum([ distmats_Y[samp_factor][i,j]\n",
    "                 for group in np.unique(agnes.labels_)\n",
    "                 for i in np.where(agnes.labels_ == group)[0]\n",
    "                 for j in np.where(agnes.labels_ == group)[0] ])\n",
    "    \n",
    "        # homogenous clusters\n",
    "        if heterogenity == 0: break\n",
    "    print(samp_factor, \":\")\n",
    "    print(\" ->\", k, \"classes:\", *[[versions[i] for i in np.where(agnes.labels_ == cl)[0]] for cl in np.unique(agnes.labels_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaed56",
   "metadata": {},
   "source": [
    "Okay, seems we start from the jpegs that are identical. That is good because we actually used the same version to create them. It would be wierd otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster by sampling factor\n",
    "for samp_factor in samp_factors:\n",
    "    for k in range(1,6):\n",
    "        agnes = AgglomerativeClustering(n_clusters=k, linkage='average', affinity='precomputed')\n",
    "        agnes.fit(distmats_rgb[samp_factor])\n",
    "    \n",
    "        # compute heterogenity metric (sum of distances)\n",
    "        heterogenity = np.sum([ distmats_rgb[samp_factor][i,j]\n",
    "                 for group in np.unique(agnes.labels_)\n",
    "                 for i in np.where(agnes.labels_ == group)[0]\n",
    "                 for j in np.where(agnes.labels_ == group)[0] ])\n",
    "    \n",
    "        # homogenous clusters\n",
    "        if heterogenity == 0: break\n",
    "    print(samp_factor, \":\")\n",
    "    print(\" ->\", k, \"classes:\", *[[versions[i] for i in np.where(agnes.labels_ == cl)[0]] for cl in np.unique(agnes.labels_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcf277",
   "metadata": {},
   "source": [
    "### Differ\n",
    "\n",
    "= differs when we half at least one of the chroma in the second dimension while first stays the same\n",
    "\n",
    "#### Downsampled only along second dimension\n",
    "\n",
    "- 12 11 11\n",
    "- 12 12 11, 12 11 12\n",
    "\n",
    "#### Half the second dimension and the first not\n",
    "\n",
    "- 22 21 11, 22 11 21 = half second in one chroma and both in the other\n",
    "- 22 22 21, 22 21 22 = half second in one chroma\n",
    "\n",
    "### Same\n",
    "\n",
    "#### Downsampled only along first dimension\n",
    "\n",
    "- 21 11 11\n",
    "- 21 21 11, 21 11 21\n",
    "\n",
    "#### Half the first dimension and the second not\n",
    "\n",
    "- 22 12 11, 22 11 12 = half first in one chroma and both on the other\n",
    "- 22 22 12, 22 12 22 = half first in one chroma\n",
    "\n",
    "\n",
    "#### Along both dimensions the same way\n",
    "\n",
    "- 22 11 11\n",
    "- 22 22 11, 22 11 22\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6bb2c",
   "metadata": {},
   "source": [
    "= quartering and thirding in second dimension seems fine\n",
    "\n",
    "= but for 14 12 11 we see a difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "import seaborn as sns\n",
    "p = sns.heatmap(\n",
    "    pd.DataFrame(distmats_rgb_cdFU_sf[((2,1),(1,1),(1,1))] == 0, index=versions2, columns=versions2),\n",
    "    linewidth=.05, ax=ax, cmap=['#BBB','#DDD'])#['lightsalmon','lightgreen'])\n",
    "colorbar = ax.collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin\n",
    "colorbar.set_ticks([colorbar.vmin + r / 2 * (0.5 + i) for i in range(2)])\n",
    "colorbar.set_ticklabels(['mismatch','match'])\n",
    "plt.savefig('../text/forensic/figures/mismatch_fancyupsampling_21-11-11.png', dpi=100);\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "import seaborn as sns\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(distmats_rgb_cdFU_sf[((1,2),(1,1),(1,1))] == 0, index=versions2, columns=versions2),\n",
    "    linewidth=.05, ax=ax, cmap=['#BBB','#DDD'])#['lightsalmon','lightgreen'])\n",
    "colorbar = ax.collections[0].colorbar \n",
    "r = colorbar.vmax - colorbar.vmin\n",
    "colorbar.set_ticks([colorbar.vmin + r / 2 * (0.5 + i) for i in range(2)])\n",
    "colorbar.set_ticklabels(['mismatch','match'])\n",
    "plt.savefig('../text/forensic/figures/mismatch_fancyupsampling_12-11-11.png', dpi=100);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c96d5",
   "metadata": {},
   "source": [
    "There is mismatch \n",
    "\n",
    "- no upsampling: 9a+ mismatch = we observe at all times (probably not because of chroma subsampling)\n",
    "- subsampling along second dimension = we observe 9a+, 7+, 6b and turbo (**WOOW111**!)\n",
    "- subsampling along the first dimension = we observe 9a+, 7+, 6b+turbo\n",
    "- subsampling along any dimension only one chroma = we observe 9a+, 7+, 6b and turbo (**WOOW222**!)\n",
    "- for factor 4 = we see 9a+, 7+, 6b+turbo\n",
    "- for factor 3 = we see 9a+ only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcdcbe-9abf-4fdf-8a85-73fd9dba2e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241be8b-eb0f-45e0-8768-5d699e28c5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2423eec-119c-444d-bfd2-888bb4ee4847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
